"""å‘¨æœŸåˆ†æé¡µé¢ â€” AI é©±åŠ¨çš„çœŸå®æ•°æ®å‘¨æœŸç ”åˆ¤"""

from __future__ import annotations

import json
import re
import threading
import time
from datetime import datetime
from pathlib import Path

import plotly.graph_objects as go
import streamlit as st

from quant.llm.client import SiliconFlowClient, LLMConfig, LLMError
from quant.llm.prompts import (
    build_cycle_analysis_prompt,
    build_cycle_chat_messages,
)
from quant.dashboard.llm_settings import (
    get_llm_config,
    get_tavily_key,
    get_jina_key,
    get_apify_key,
)
from quant.research.data_fetcher import DataFetcher, _dedupe_news as dedupe_news
from quant.research.tavily_client import TavilyClient
from quant.research.jina_reader import JinaReader
from quant.research.apify_client import ApifyClient

# ==================== è·¯å¾„å¸¸é‡ ====================

_ROOT = Path(__file__).parent.parent.parent
CYCLE_DATA_PATH = _ROOT / "data" / "cycle_analysis.json"
FACTORS_PATH = _ROOT / "data" / "sector_factors.json"
RESEARCH_DIR = _ROOT / "data" / "research"


# ==================== æŒä¹…åŒ– ====================

def _load_all_cycles() -> dict:
    if CYCLE_DATA_PATH.exists():
        try:
            with open(CYCLE_DATA_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            pass
    return {}


def _save_all_cycles(data: dict) -> None:
    CYCLE_DATA_PATH.parent.mkdir(parents=True, exist_ok=True)
    with open(CYCLE_DATA_PATH, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def _save_cycle(sector: str, cycle_data: dict) -> None:
    all_data = _load_all_cycles()
    all_data[sector] = cycle_data
    _save_all_cycles(all_data)


def _load_all_factors() -> dict:
    """ä» sector_factors.json åŠ è½½æ‰€æœ‰æ¿å—å› å­"""
    if FACTORS_PATH.exists():
        try:
            with open(FACTORS_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            pass
    return {}


def _safe_dirname(name: str) -> str:
    """æ¿å—åè½¬å®‰å…¨ç›®å½•å"""
    return re.sub(r'[\\/:*?"<>|]', "_", name)


# ==================== JSON æå– ====================

def extract_json_from_text(text: str) -> dict | None:
    """ä» LLM å›å¤ä¸­æå– JSON"""
    # 1) ```json ... ```
    m = re.search(r"```json\s*\n?(.*?)\n?\s*```", text, re.DOTALL)
    if m:
        try:
            return json.loads(m.group(1))
        except json.JSONDecodeError:
            pass
    # 2) è£¸ JSON
    m = re.search(r"\{[\s\S]*\"overall\"[\s\S]*\}", text)
    if m:
        try:
            return json.loads(m.group(0))
        except json.JSONDecodeError:
            pass
    return None


# ==================== å›¾è¡¨ ====================

def _safe_float(val) -> float | None:
    """å®‰å…¨è½¬ floatï¼Œéæ•°å­—è¿”å› None"""
    if val is None:
        return None
    if isinstance(val, (int, float)):
        return float(val)
    if isinstance(val, str):
        # å»æ‰å¸¸è§å¹²æ‰°å­—ç¬¦ï¼Œåªä¿ç•™æ•°å­—å’Œå°æ•°ç‚¹
        cleaned = re.sub(r"[^\d.\-]", "", val)
        if cleaned:
            try:
                return float(cleaned)
            except ValueError:
                pass
    return None


def _quarter_to_datetime(q_str: str) -> datetime | None:
    """'2022-Q1' â†’ datetime(2022, 2, 15)ï¼Œæ”¯æŒ YYYY-QN æ ¼å¼"""
    try:
        parts = q_str.split("-")
        year = int(parts[0])
        q = int(parts[1][1]) if len(parts) > 1 and parts[1].startswith("Q") else 1
        month = {1: 2, 2: 5, 3: 8, 4: 11}.get(q, 2)
        return datetime(year, month, 15)
    except (ValueError, IndexError, KeyError):
        return None


def _build_factor_chart(factor: dict) -> go.Figure:
    """æ„å»ºå› å­å‘¨æœŸå›¾è¡¨ â€” å…³é”®èŠ‚ç‚¹è¿çº¿ï¼ŒXè½´ç­‰æ¯”æ—¶é—´"""
    fig = go.Figure()
    all_points = []  # (date_str, datetime, value, type)

    for cycle in factor.get("cycle_data", []):
        peak = cycle.get("peak", {})
        trough = cycle.get("trough", {})
        peak_val = _safe_float(peak.get("value"))
        trough_val = _safe_float(trough.get("value"))
        if peak.get("date") and peak_val is not None:
            all_points.append((peak["date"], peak_val, "peak"))
        if trough.get("date") and trough_val is not None:
            all_points.append((trough["date"], trough_val, "trough"))

        # æå– key_pointsï¼ˆä¸­é—´å…³é”®èŠ‚ç‚¹ï¼‰
        for kp in cycle.get("key_points", []):
            kp_val = _safe_float(kp.get("value"))
            if kp.get("date") and kp_val is not None:
                all_points.append((kp["date"], kp_val, "normal"))

    if not all_points:
        return fig

    # å»é‡ï¼ˆåŒæ—¥æœŸåŒå€¼åªä¿ç•™ä¼˜å…ˆçº§æœ€é«˜çš„ç±»å‹ï¼špeak > trough > normalï¼‰
    seen = {}
    priority = {"peak": 0, "trough": 1, "normal": 2}
    for date_str, val, ptype in all_points:
        key = date_str
        if key not in seen or priority.get(ptype, 2) < priority.get(seen[key][2], 2):
            seen[key] = (date_str, val, ptype)
    all_points = list(seen.values())

    # æŒ‰æ—¥æœŸæ’åº
    all_points.sort(key=lambda x: x[0])

    # è½¬æ¢ä¸º datetime ç”¨äºç­‰æ¯” X è½´
    dt_points = []
    for date_str, val, ptype in all_points:
        dt = _quarter_to_datetime(date_str)
        if dt:
            dt_points.append((dt, date_str, val, ptype))

    if not dt_points:
        return fig

    dates_dt = [p[0] for p in dt_points]
    date_labels = [p[1] for p in dt_points]
    values = [p[2] for p in dt_points]
    types = [p[3] for p in dt_points]

    # ä¸»æŠ˜çº¿
    fig.add_trace(go.Scatter(
        x=dates_dt, y=values,
        mode="lines+markers",
        name="å‘¨æœŸèµ°åŠ¿",
        line=dict(color="#5B8DEF", width=2),
        marker=dict(size=6),
        text=date_labels,
        hovertemplate="%{text}<br>%{y}<extra></extra>",
    ))

    # é«˜ç‚¹æ ‡æ³¨ï¼ˆçº¢è‰²å€’ä¸‰è§’ï¼‰
    peaks = [(dt, lbl, v) for dt, lbl, v, t in dt_points if t == "peak"]
    if peaks:
        fig.add_trace(go.Scatter(
            x=[p[0] for p in peaks],
            y=[p[2] for p in peaks],
            mode="markers+text",
            name="é«˜ç‚¹",
            marker=dict(color="red", size=12, symbol="triangle-down"),
            text=[f"{v}" for _, _, v in peaks],
            textposition="top center",
            textfont=dict(size=11, color="red"),
        ))

    # ä½ç‚¹æ ‡æ³¨ï¼ˆç»¿è‰²æ­£ä¸‰è§’ï¼‰
    troughs = [(dt, lbl, v) for dt, lbl, v, t in dt_points if t == "trough"]
    if troughs:
        fig.add_trace(go.Scatter(
            x=[p[0] for p in troughs],
            y=[p[2] for p in troughs],
            mode="markers+text",
            name="ä½ç‚¹",
            marker=dict(color="green", size=12, symbol="triangle-up"),
            text=[f"{v}" for _, _, v in troughs],
            textposition="bottom center",
            textfont=dict(size=11, color="green"),
        ))

    unit = factor.get("unit", "")
    fig.update_layout(
        title=f"{factor['name']}" + (f" ({unit})" if unit else ""),
        yaxis_title=unit,
        hovermode="x unified",
        showlegend=False,
        height=350,
        margin=dict(l=40, r=20, t=40, b=30),
    )
    fig.update_xaxes(dtick="M12", tickformat="%Y")
    return fig


# ==================== æ¸²æŸ“ç»„ä»¶ ====================

def _render_cycle_result(sector: str, data: dict) -> None:
    """æ¸²æŸ“åˆ†æç»“æœï¼šç»¼åˆåˆ¤æ–­ + å› å­å›¾è¡¨ + è¡Œä¸šèµ„è®¯"""
    overall = data.get("overall", {})

    # ç»¼åˆåˆ¤æ–­é¢æ¿
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown(f"### {overall.get('cycle_position', 'æœªçŸ¥')}")
        st.caption("å‘¨æœŸä½ç½®")
    with col2:
        prob = overall.get("reversal_probability", 0)
        timeframe = overall.get("probability_timeframe", "12ä¸ªæœˆ")
        rationale = overall.get("probability_rationale", "")
        st.metric(f"åè½¬æ¦‚ç‡({timeframe})", f"{prob}%")
        if rationale:
            st.caption(rationale)
    with col3:
        st.markdown("**å…³é”®ä¿¡å·**")
        for s in overall.get("key_signals", []):
            st.markdown(f"- {s}")

    summary = overall.get("summary", "")
    if summary:
        st.markdown(summary)

    # æ¯ä¸ªå› å­çš„å›¾è¡¨
    for factor in data.get("factors", []):
        st.markdown(f"#### {factor['name']}")

        if factor.get("cycle_data"):
            fig = _build_factor_chart(factor)
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info(f"ã€Œ{factor['name']}ã€æ•°æ®ä¸è¶³ï¼Œæœªèƒ½ç”Ÿæˆå‘¨æœŸå›¾è¡¨ã€‚")

        # å› å­å°ç»“
        cols = st.columns([2, 2, 1])
        with cols[0]:
            avg = factor.get("avg_cycle_length_months")
            if avg:
                st.caption(f"å¹³å‡å‘¨æœŸé•¿åº¦: {avg} ä¸ªæœˆ")
            current_pos = factor.get("current_position", "æœªçŸ¥")
            current_val = factor.get("current_value", "")
            if current_val:
                st.caption(f"å½“å‰å€¼: {current_val} | ä½ç½®: {current_pos}")
            else:
                st.caption(f"å½“å‰ä½ç½®: {current_pos}")
        with cols[1]:
            confidence = factor.get("data_confidence", "unknown")
            confidence_label = {"high": "é«˜", "medium": "ä¸­", "low": "ä½"}.get(
                confidence, "æœªçŸ¥"
            )
            source_url = factor.get("data_source_url", "")
            st.caption(f"æ•°æ®å¯ä¿¡åº¦: {confidence_label}")
            if source_url:
                short_url = source_url[:50] + ("..." if len(source_url) > 50 else "")
                st.caption(f"[{short_url}]({source_url})")
        with cols[2]:
            archive = data.get("archive_path", "")
            if archive:
                st.caption(f"åŸå§‹æ•°æ®å·²å½’æ¡£")

        analysis = factor.get("analysis", "")
        if analysis:
            st.markdown(analysis)

    # è¡Œä¸šèµ„è®¯
    news = data.get("news", [])
    if news:
        st.markdown("#### è¡Œä¸šèµ„è®¯")
        for item in news[:8]:
            title = item.get("title", "")
            url = item.get("url", "")
            snippet = item.get("snippet", "")[:100]
            if title and url:
                st.markdown(f"- [{title}]({url})" + (f" â€” {snippet}" if snippet else ""))


def _render_cycle_chat(
    sector: str, llm_config: LLMConfig, cycle_data: dict,
) -> None:
    """å¯¹è¯åŒºï¼šè®¨è®º/è°ƒæ•´å‘¨æœŸåˆ¤æ–­"""
    msg_key = f"cycle_chat_{sector}_messages"

    if msg_key not in st.session_state:
        st.session_state[msg_key] = cycle_data.get("conversation", [])

    messages: list[dict] = st.session_state[msg_key]

    # å±•ç¤ºå†å²
    for msg in messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    pending_key = f"_pending_cycle_chat_{sector}"
    user_input = st.chat_input(
        f"è®¨è®ºã€Œ{sector}ã€çš„å‘¨æœŸ...", key=f"cycle_chat_input_{sector}",
    )
    if user_input:
        messages.append({"role": "user", "content": user_input})
        st.session_state[pending_key] = True
        st.rerun()

    if not st.session_state.get(pending_key):
        return

    # æ„å»º LLM è¯·æ±‚
    last_user_msg = messages[-1]["content"] if messages and messages[-1]["role"] == "user" else ""
    llm_messages = build_cycle_chat_messages(sector, cycle_data, messages[:-1], last_user_msg)

    llm_cfg = LLMConfig(
        api_key=llm_config.api_key,
        base_url=llm_config.base_url,
        model=llm_config.model,
        max_tokens=4096,
    )

    from quant.dashboard.bg_task import bg_llm_stream, clear_task
    task_id = f"cycle_chat_{sector}"
    with st.chat_message("assistant"):
        full_reply = bg_llm_stream(task_id, llm_cfg, llm_messages, retry_key=f"retry_cycle_chat_{sector}")

    if full_reply is None:
        return

    messages.append({"role": "assistant", "content": full_reply})

    if len(messages) > 40:
        messages = messages[-40:]
    st.session_state[msg_key] = messages

    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ›´æ–°çš„ JSON
    parsed = extract_json_from_text(full_reply)
    if parsed and "overall" in parsed:
        parsed["conversation"] = messages
        parsed["analyzed_at"] = cycle_data.get("analyzed_at", "")
        parsed["archive_path"] = cycle_data.get("archive_path", "")
        parsed["news"] = cycle_data.get("news", [])
        _save_cycle(sector, parsed)
        st.session_state.pop(pending_key, None)
        clear_task(task_id)
        st.rerun()

    # æŒä¹…åŒ–å¯¹è¯
    cycle_data["conversation"] = messages
    _save_cycle(sector, cycle_data)
    st.session_state.pop(pending_key, None)
    clear_task(task_id)


# ==================== åˆ†æ­¥æ¸²æŸ“ ====================

def _render_step(step: str, status: str, detail) -> None:
    """æ¸²æŸ“æ•°æ®è·å–çš„å•æ­¥ç»“æœï¼ˆLemonAI é£æ ¼ï¼‰"""

    if step == "guidance" and status == "start":
        st.caption("ğŸ¤– è¯¢é—® AI æ•°æ®å»å“ªæ‰¾...")

    elif step == "guidance" and status == "done":
        with st.expander("ğŸ¤– AI æ•°æ®æºæŒ‡å¯¼", expanded=False):
            if detail.get("search_queries"):
                st.markdown("**æœç´¢å…³é”®è¯:**")
                for q in detail["search_queries"]:
                    st.markdown(f"- `{q}`")
            if detail.get("suggested_urls"):
                st.markdown("**å»ºè®®æ•°æ®æº:**")
                for url in detail["suggested_urls"]:
                    st.markdown(f"- {url}")
            if detail.get("akshare_api"):
                st.markdown(f"**AKShare æ¥å£:** `{detail['akshare_api']}`")

    elif step == "akshare" and status == "start":
        st.caption(f"ğŸ“¡ å°è¯• AKShare: `{detail.get('api', '')}`...")

    elif step == "akshare" and status == "done":
        n = detail.get("records", 0)
        st.markdown(f"ğŸ“¡ AKShare: âœ… è·å–åˆ° {n} æ¡è®°å½•")

    elif step == "akshare" and status == "fail":
        st.markdown("ğŸ“¡ AKShare: âŒ æœªæ‰¾åˆ°å¯¹åº”æ•°æ®")

    elif step == "akshare" and status == "skip":
        pass  # é™é»˜è·³è¿‡

    elif step == "tavily" and status == "start":
        queries = detail.get("queries", []) if isinstance(detail, dict) else []
        st.caption(f"ğŸ” æœç´¢ä¸­... ({len(queries)} ç»„å…³é”®è¯)")

    elif step == "tavily" and status == "done":
        results = detail if isinstance(detail, list) else []
        st.markdown(f"ğŸ” æœç´¢ç»“æœ: æ‰¾åˆ° **{len(results)}** æ¡")
        if results:
            with st.expander(f"æŸ¥çœ‹æœç´¢ç»“æœ ({len(results)} æ¡)", expanded=False):
                for j, r in enumerate(results[:8]):
                    title = r.get("title", "æ— æ ‡é¢˜")
                    url = r.get("url", "")
                    snippet = (r.get("content", "") or "")[:120]
                    st.markdown(f"**[{j+1}]** [{title}]({url})")
                    if snippet:
                        st.caption(snippet)

    elif step == "tavily" and status == "fail":
        st.markdown("ğŸ” æœç´¢ç»“æœ: âŒ æœªæ‰¾åˆ°ç›¸å…³æ•°æ®")

    elif step == "jina" and status == "start":
        url = (detail.get("url", "") if isinstance(detail, dict) else "")[:50]
        st.caption(f"ğŸ“„ ç½‘é¡µæŠ“å–: {url}...")

    elif step == "jina" and status == "done":
        chars = detail.get("chars", 0) if isinstance(detail, dict) else 0
        url = (detail.get("url", "") if isinstance(detail, dict) else "")[:50]
        st.markdown(f"ğŸ“„ ç½‘é¡µæŠ“å–: âœ… æˆåŠŸ ({chars} å­—) â€” {url}...")

    elif step == "jina" and status == "fail":
        url = (detail.get("url", "") if isinstance(detail, dict) else "")[:50]
        st.markdown(f"ğŸ“„ ç½‘é¡µæŠ“å–: âŒ å¤±è´¥ â€” {url}")

    elif step == "apify" and status == "start":
        url = (detail.get("url", "") if isinstance(detail, dict) else "")[:50]
        st.caption(f"ğŸ•·ï¸ Apify çˆ¬è™«: {url}...")

    elif step == "apify" and status == "done":
        chars = detail.get("chars", 0) if isinstance(detail, dict) else 0
        st.markdown(f"ğŸ•·ï¸ Apify çˆ¬è™«: âœ… æˆåŠŸ ({chars} å­—)")

    elif step == "apify" and status == "fail":
        st.markdown("ğŸ•·ï¸ Apify çˆ¬è™«: âŒ å¤±è´¥")


# ==================== åå°åˆ†ææµç¨‹ ====================

def _bg_analysis_worker(
    sector: str,
    factors: list[dict],
    llm_config: LLMConfig,
    progress: dict,
    cancel_event: threading.Event,
) -> None:
    """åå°çº¿ç¨‹æ‰§è¡Œåˆ†æ â€” ç¦æ­¢ä»»ä½• st.* è°ƒç”¨"""
    try:
        tavily_key = get_tavily_key()
        jina_key = get_jina_key()
        apify_key = get_apify_key()

        top_factors = sorted(
            factors, key=lambda f: f.get("weight", 0), reverse=True,
        )

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        archive_dir = RESEARCH_DIR / _safe_dirname(sector) / timestamp

        fetcher = DataFetcher(
            llm_client=SiliconFlowClient(llm_config),
            tavily=TavilyClient(tavily_key) if tavily_key else None,
            jina=JinaReader(jina_key) if jina_key else None,
            apify=ApifyClient(apify_key) if apify_key else None,
        )

        factor_data_map = {}
        all_news = []

        for i, f in enumerate(top_factors):
            if cancel_event.is_set():
                progress["status"] = "cancelled"
                return

            progress["current_factor"] = f["name"]
            progress["factor_index"] = i + 1
            progress["factor_total"] = len(top_factors)
            progress["current_step"] = "fetch"
            progress["log"].append(
                f"({i+1}/{len(top_factors)}) å¼€å§‹è·å–ã€Œ{f['name']}ã€æ•°æ®..."
            )

            def on_step(step, status, detail, _name=f["name"]):
                if step == "guidance" and status == "done":
                    progress["log"].append(f"  AI æ•°æ®æºæŒ‡å¯¼å®Œæˆ")
                elif step == "tavily" and status == "done":
                    n = len(detail) if isinstance(detail, list) else 0
                    progress["log"].append(f"  æœç´¢å®Œæˆ: {n} æ¡ç»“æœ")
                elif step == "jina" and status == "done":
                    chars = detail.get("chars", 0) if isinstance(detail, dict) else 0
                    progress["log"].append(f"  ç½‘é¡µæŠ“å–: {chars} å­—")
                elif step == "akshare" and status == "done":
                    n = detail.get("records", 0)
                    progress["log"].append(f"  AKShare: {n} æ¡è®°å½•")

            result = fetcher.fetch_factor_data(sector, f, progress_callback=on_step)
            factor_data_map[f["name"]] = result
            all_news.extend(result.news)

            # è®°å½•å› å­è¯¦æƒ…ä¾› UI å±•ç¤º
            progress["factor_details"].append({
                "name": f["name"],
                "weight": f.get("weight", 0),
                "status": "completed" if result.found else "failed",
                "source": result.source,
                "guidance": result.guidance,
                "search_results": result.search_results_raw[:5],
                "fetch_log": result.fetch_log,
            })

            if result.found:
                progress["log"].append(
                    f"ã€Œ{f['name']}ã€æ•°æ®è·å–å®Œæˆ â€” {result.source}"
                )
            else:
                progress["log"].append(f"ã€Œ{f['name']}ã€æœªæ‰¾åˆ°æ•°æ®")

        if cancel_event.is_set():
            progress["status"] = "cancelled"
            return

        fetcher.save_archive(archive_dir)

        has_data = any(r.found for r in factor_data_map.values())
        if not has_data:
            progress["status"] = "failed"
            progress["error"] = "æ‰€æœ‰å› å­å‡æœªæ‰¾åˆ°å¯ç”¨æ•°æ®ï¼Œæ— æ³•è¿›è¡Œå‘¨æœŸåˆ†æã€‚"
            return

        # LLM åˆ†æ
        progress["current_step"] = "llm"
        progress["log"].append("AI æ­£åœ¨åˆ†æå‘¨æœŸ...")
        messages = build_cycle_analysis_prompt(sector, top_factors, factor_data_map)

        client = SiliconFlowClient(LLMConfig(
            api_key=llm_config.api_key,
            base_url=llm_config.base_url,
            model=llm_config.model,
            max_tokens=4096,
        ))

        full_reply = client.chat(messages)

        if cancel_event.is_set():
            progress["status"] = "cancelled"
            return

        # ä¿å­˜ LLM åŸæ–‡
        try:
            archive_dir.mkdir(parents=True, exist_ok=True)
            (archive_dir / "llm_response.txt").write_text(full_reply, encoding="utf-8")
        except Exception:
            pass

        parsed = extract_json_from_text(full_reply)
        if parsed and "overall" in parsed:
            parsed["news"] = dedupe_news(all_news)
            parsed["conversation"] = []
            parsed["analyzed_at"] = datetime.now().isoformat(timespec="seconds")
            parsed["archive_path"] = str(archive_dir)
            _save_cycle(sector, parsed)
            progress["result"] = parsed
            progress["status"] = "completed"
            progress["log"].append("åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜ã€‚")
        else:
            progress["status"] = "failed"
            progress["error"] = "AI æœªè¿”å›æœ‰æ•ˆçš„åˆ†æç»“æœ"
            progress["llm_tail"] = full_reply[-500:] if full_reply else ""
            progress["log"].append("AI æœªè¿”å›æœ‰æ•ˆç»“æœ")

    except Exception as e:
        progress["status"] = "failed"
        progress["error"] = str(e)
        progress["log"].append(f"åˆ†æå‡ºé”™: {e}")


def _start_bg_analysis(
    sector: str, factors: list[dict], llm_config: LLMConfig,
) -> None:
    """å¯åŠ¨åå°åˆ†æçº¿ç¨‹"""
    cancel_event = threading.Event()
    progress = {
        "sector": sector,
        "status": "running",
        "current_step": "init",
        "current_factor": "",
        "factor_index": 0,
        "factor_total": 0,
        "log": [],
        "factor_details": [],
        "result": None,
        "error": None,
        "llm_tail": "",
        "started_at": datetime.now().strftime("%H:%M:%S"),
    }

    thread = threading.Thread(
        target=_bg_analysis_worker,
        args=(sector, factors, llm_config, progress, cancel_event),
        daemon=True,
    )

    st.session_state["bg_analysis"] = {
        "thread": thread,
        "cancel_event": cancel_event,
        "progress": progress,
    }

    thread.start()


def _render_bg_status() -> bool:
    """æ¸²æŸ“åå°åˆ†æçŠ¶æ€ï¼Œè¿”å›æ˜¯å¦æ­£åœ¨è¿è¡Œ"""
    bg = st.session_state.get("bg_analysis")
    if not bg:
        return False

    progress = bg["progress"]
    status = progress["status"]
    sector = progress["sector"]

    if status == "running":
        with st.container(border=True):
            # æ ‡é¢˜è¡Œ + ç»ˆæ­¢æŒ‰é’®
            col_title, col_btn = st.columns([4, 1])
            with col_title:
                if progress["current_step"] == "llm":
                    step_text = "AI æ­£åœ¨åˆ†æå‘¨æœŸ..."
                elif progress["factor_total"] > 0:
                    step_text = (
                        f"è·å–æ•°æ® ({progress['factor_index']}/{progress['factor_total']})"
                        f" â€” {progress.get('current_factor', '')}"
                    )
                else:
                    step_text = "åˆå§‹åŒ–..."
                st.markdown(f"**æ­£åœ¨åˆ†æã€Œ{sector}ã€** â€” {step_text}")
            with col_btn:
                if st.button("ç»ˆæ­¢åˆ†æ", type="secondary", key="stop_analysis"):
                    bg["cancel_event"].set()
                    progress["status"] = "cancelling"
                    st.rerun()

            # è¿›åº¦æ—¥å¿—ï¼ˆæœ€è¿‘ 10 æ¡ï¼‰
            for msg in progress["log"][-10:]:
                st.caption(msg)

            # å› å­è·å–è¯¦æƒ…å¡ç‰‡
            factor_details = progress.get("factor_details", [])
            if factor_details:
                st.markdown("**å„å› å­è·å–è¯¦æƒ…:**")
                for detail in factor_details:
                    status_icon = "âœ…" if detail["status"] == "completed" else "âŒ"
                    with st.expander(
                        f"{status_icon} {detail['name']} ({detail['weight']}%) â€” "
                        f"{detail.get('source', 'æœªè·å–æ•°æ®') or 'æœªè·å–æ•°æ®'}",
                        expanded=False,
                    ):
                        # AI æ•°æ®æºæŒ‡å¯¼
                        guidance = detail.get("guidance", {})
                        if guidance:
                            st.markdown("**AI å»ºè®®çš„æœç´¢å…³é”®è¯:**")
                            for q in guidance.get("search_queries", []):
                                st.code(q, language="text")
                            if guidance.get("suggested_urls"):
                                st.markdown("**å»ºè®®æ•°æ®æº:**")
                                for url in guidance["suggested_urls"]:
                                    st.markdown(f"- {url}")
                            if guidance.get("akshare_api"):
                                st.code(f"AKShare: {guidance['akshare_api']}", language="python")

                        # æœç´¢ç»“æœ
                        search_results = detail.get("search_results", [])
                        if search_results:
                            st.markdown(f"**æœç´¢ç»“æœ:** {len(search_results)} æ¡")
                            for j, r in enumerate(search_results[:3]):
                                title = r.get("title", "æ— æ ‡é¢˜")
                                url = r.get("url", "")
                                snippet = (r.get("content", "") or "")[:120]
                                st.markdown(f"**[{j+1}]** [{title}]({url})")
                                if snippet:
                                    st.caption(snippet)

                        # è·å–æ—¥å¿—ï¼ˆæˆåŠŸ/å¤±è´¥è®°å½•ï¼‰
                        fetch_log = detail.get("fetch_log", [])
                        if fetch_log:
                            st.markdown("**æ•°æ®è·å–è®°å½•:**")
                            for log_item in fetch_log:
                                icon = "âœ…" if log_item.get("success") else "âŒ"
                                method = log_item.get("method", "unknown")
                                url = (log_item.get("url", "") or "")[:60]
                                chars = log_item.get("chars", 0)
                                st.caption(f"{icon} {method} â€” {url} ({chars} å­—)")

        return True

    if status == "cancelling":
        st.warning(f"æ­£åœ¨ç»ˆæ­¢ã€Œ{sector}ã€åˆ†æï¼Œè¯·ç¨å€™...")
        # æ£€æŸ¥çº¿ç¨‹æ˜¯å¦å·²ç»“æŸ
        if not bg["thread"].is_alive():
            progress["status"] = "cancelled"
            st.rerun()
        return True

    if status == "completed":
        st.success(f"ã€Œ{sector}ã€åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜ã€‚")
        if st.button("å…³é—­æç¤º", key="dismiss_complete"):
            del st.session_state["bg_analysis"]
            st.rerun()
        return False

    if status == "cancelled":
        st.warning(f"ã€Œ{sector}ã€åˆ†æå·²ç»ˆæ­¢ã€‚")
        if st.button("å…³é—­", key="dismiss_cancel"):
            del st.session_state["bg_analysis"]
            st.rerun()
        return False

    if status == "failed":
        st.error(f"ã€Œ{sector}ã€åˆ†æå¤±è´¥: {progress.get('error', 'æœªçŸ¥é”™è¯¯')}")
        if progress.get("llm_tail"):
            with st.expander("æŸ¥çœ‹ AI åŸå§‹è¾“å‡º"):
                st.code(progress["llm_tail"])
        if st.button("å…³é—­", key="dismiss_fail"):
            del st.session_state["bg_analysis"]
            st.rerun()
        return False

    return False


# ==================== ä¸»å…¥å£ ====================

def render_cycle_analysis_page() -> None:
    """æ¸²æŸ“ã€Œå‘¨æœŸåˆ†æã€é¡µé¢"""
    st.header("å‘¨æœŸåˆ†æ")

    llm_config = get_llm_config()
    if not llm_config:
        st.warning("è¯·å…ˆåœ¨ã€Œè®¾ç½®ã€é¡µé¢é…ç½® LLM API Keyã€‚")
        return

    # åå°åˆ†æçŠ¶æ€ï¼ˆé¡¶éƒ¨æ¨ªå¹…ï¼‰
    is_running = _render_bg_status()

    # 1. å·²åˆ†æå‘¨æœŸåˆ—è¡¨
    _render_saved_cycles(llm_config)

    # 2. æ–°å»ºåˆ†æ
    st.markdown("---")
    st.subheader("æ–°å»ºå‘¨æœŸåˆ†æ")

    # ä» sector_factors.json åŠ è½½æœ‰å› å­çš„æ¿å—
    all_factors = _load_all_factors()
    sectors_with_factors = {
        k: v for k, v in all_factors.items() if v.get("factors")
    }

    if not sectors_with_factors:
        st.info("æš‚æ— å·²åˆ†æçš„æ¿å—å› å­ã€‚è¯·å…ˆåœ¨ã€Œæ¿å—åŠå› å­ã€é¡µé¢ç”Ÿæˆå› å­ã€‚")
        return

    sector_names = list(sectors_with_factors.keys())
    selected_sector = st.selectbox(
        "é€‰æ‹©æ¿å—",
        sector_names,
        help="ä»ã€Œæ¿å—åŠå› å­ã€é¡µå·²åˆ†æçš„æ¿å—ä¸­é€‰æ‹©",
    )

    if selected_sector:
        factors = sectors_with_factors[selected_sector].get("factors", [])
        # æ˜¾ç¤ºå°†åˆ†æçš„å› å­
        top_factors = sorted(factors, key=lambda f: f.get("weight", 0), reverse=True)
        factor_summary = " / ".join(
            f"{f['name']} ({f['weight']}%)" for f in top_factors
        )
        st.caption(f"å°†åˆ†æå› å­: {factor_summary}")

        # æ£€æŸ¥æ˜¯å¦æœ‰å·²æœ‰åˆ†æ
        saved = _load_all_cycles()
        if selected_sector in saved:
            st.caption(
                f"è¯¥æ¿å—å·²æœ‰åˆ†æç»“æœï¼ˆ{saved[selected_sector].get('analyzed_at', '')[:10]}ï¼‰ï¼Œ"
                "é‡æ–°åˆ†æå°†è¦†ç›–æ—§æ•°æ®ã€‚"
            )

        # TODO æ¸…å• â€” å±•ç¤ºåˆ†æè®¡åˆ’
        with st.container(border=True):
            st.markdown("**åˆ†æè®¡åˆ’ (TODO)**")
            st.markdown(f"1. è¯¢é—® AI æ¯ä¸ªå› å­çš„æ•°æ®æºå»ºè®®")
            for j, tf in enumerate(top_factors):
                st.markdown(f"2.{j+1} è·å–ã€Œ{tf['name']}ã€({tf['weight']}%) æ•°æ® â€” AKShare â†’ Tavily â†’ Jina â†’ Apify")
            st.markdown(f"{len(top_factors)+2}. AI ç»¼åˆåˆ†ææ‰€æœ‰å› å­å‘¨æœŸ")
            st.markdown(f"{len(top_factors)+3}. ä¿å­˜ç»“æœåˆ° cycle_analysis.json")

        if st.button("å¼€å§‹åˆ†æ", type="primary", disabled=is_running):
            _start_bg_analysis(selected_sector, factors, llm_config)
            st.rerun()

    # åå°è¿è¡Œä¸­ â†’ å®šæ—¶åˆ·æ–°
    if is_running:
        time.sleep(2)
        st.rerun()


def _render_data_integration_summary(data: dict) -> None:
    """å±•ç¤º AI å¦‚ä½•æ•´åˆå¤šæºæ•°æ®çš„æ¦‚è§ˆ"""
    factors = data.get("factors", [])
    if not factors:
        return

    st.markdown("#### æ•°æ®æ•´åˆæ¦‚è§ˆ")
    for f in factors:
        confidence = f.get("data_confidence", "unknown")
        confidence_label = {"high": "é«˜", "medium": "ä¸­", "low": "ä½"}.get(confidence, "æœªçŸ¥")
        confidence_color = {"high": "green", "medium": "orange", "low": "red"}.get(confidence, "gray")

        with st.expander(
            f":{confidence_color}[{confidence_label}å¯ä¿¡åº¦] **{f['name']}** â€” {f.get('current_position', 'æœªçŸ¥')}",
            expanded=False,
        ):
            col1, col2, col3 = st.columns(3)
            col1.metric("æ•°æ®å¯ä¿¡åº¦", confidence_label)
            col2.metric("å‘¨æœŸæ•°é‡", len(f.get("cycle_data", [])))
            avg_len = f.get("avg_cycle_length_months", 0)
            col3.metric("å¹³å‡å‘¨æœŸé•¿åº¦", f"{avg_len} æœˆ" if avg_len else "N/A")

            current_val = f.get("current_value", "")
            if current_val:
                st.markdown(f"**å½“å‰å€¼:** {current_val} {f.get('unit', '')}")

            source_url = f.get("data_source_url", "")
            if source_url:
                st.markdown(f"**æ•°æ®æ¥æº:** [{source_url[:50]}...]({source_url})")

            analysis = f.get("analysis", "")
            if analysis:
                st.markdown("**AI åˆ†ææ¨ç†:**")
                st.markdown(analysis)

    # æ•´ä½“åˆ¤æ–­ä¾æ®
    overall = data.get("overall", {})
    rationale = overall.get("probability_rationale", "")
    key_signals = overall.get("key_signals", [])
    if rationale or key_signals:
        st.markdown("#### ç»¼åˆåˆ¤æ–­ä¾æ®")
        if rationale:
            st.markdown(f"**åè½¬æ¦‚ç‡ä¾æ®:** {rationale}")
        if key_signals:
            st.markdown("**å…³é”®ä¿¡å·:**")
            for signal in key_signals:
                st.markdown(f"- {signal}")


def _render_saved_cycles(llm_config: LLMConfig) -> None:
    """æ¸²æŸ“å·²åˆ†æå‘¨æœŸ expander åˆ—è¡¨"""
    saved = _load_all_cycles()
    if not saved:
        return

    st.subheader(f"å·²åˆ†æå‘¨æœŸï¼ˆ{len(saved)}ï¼‰")

    for sector, data in saved.items():
        overall = data.get("overall", {})
        pos = overall.get("cycle_position", "æœªçŸ¥")
        prob = overall.get("reversal_probability", 0)
        analyzed_at = data.get("analyzed_at", "")
        time_str = analyzed_at[:10] if analyzed_at else ""

        label = f"**{sector}** â€” {pos} Â· åè½¬æ¦‚ç‡ {prob}% Â· {time_str}"
        with st.expander(label):
            _render_cycle_result(sector, data)
            st.markdown("---")
            _render_data_integration_summary(data)
            st.markdown("---")
            st.markdown("##### å¯¹è¯è®¨è®º")
            _render_cycle_chat(sector, llm_config, data)
